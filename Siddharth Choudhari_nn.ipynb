{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDriIbfa5lwD"
   },
   "source": [
    "Problem statement: To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvR7ppk77v31"
   },
   "source": [
    "### Importing Skin Cancer Data\n",
    "#### To do: Take necessary actions to read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfcpIXQZN2Rh"
   },
   "source": [
    "### Importing all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WC8xCQuELWms"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYpVPmT5z7AP"
   },
   "outputs": [],
   "source": [
    "## Files are loaded on local drive and then used in the code for formation of test and train numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpUsRQwOOL72"
   },
   "source": [
    "This assignment uses a dataset of about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D57L-ovIKtI4"
   },
   "outputs": [],
   "source": [
    "# Defining the path for train and test images\n",
    "## Todo: Update the paths of the train and test dataset\n",
    "trainpath = pathlib.Path(r\"D:\\Siddharth Upgrad\\Case Study\\17. Melanoma detection\\CNN_assignment\\Skin cancer ISIC The International Skin Imaging Collaboration\\Train\")\n",
    "testpath = pathlib.Path(r\"D:\\Siddharth Upgrad\\Case Study\\17. Melanoma detection\\CNN_assignment\\Skin cancer ISIC The International Skin Imaging Collaboration\\Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqksN1w5Fu-N"
   },
   "outputs": [],
   "source": [
    "## The below piece of code scans the train dataset path , reads images in individual folders and converts them to numpy arrays.\n",
    "## This forms the X_train array\n",
    "## Parallely it also writes labels to another array, which forms the y_train array.\n",
    "\n",
    "trainpathlist = list(trainpath.iterdir())\n",
    "trainclasslist = []\n",
    "for i in trainpathlist:\n",
    "    length = len(i.parts)-1\n",
    "    trainclasslist.append(i.parts[length])\n",
    "    \n",
    "subtrainpath = []\n",
    "for i in trainclasslist:\n",
    "    subtrainpath.append(pathlib.Path(str(trainpath)+'/'+ i))\n",
    "\n",
    "traininterlist = []\n",
    "listlength=0\n",
    "X_trainarray=[]\n",
    "y_trainarray = []\n",
    "for i in range(0,9):\n",
    "    traininterlist = list(subtrainpath[i].iterdir())\n",
    "    listlength = listlength + len(traininterlist)\n",
    "    print(\"Label Name = \" + str(subtrainpath[i].name))\n",
    "    print(\"Label Number = \" + str(i))\n",
    "    print(\"number of train images = \" + str(len(traininterlist)))\n",
    "    print()\n",
    "    for j in traininterlist:\n",
    "        image = tf.keras.utils.load_img(j)\n",
    "        image = image.resize((180,180))\n",
    "        X_trainarray.append(tf.keras.utils.img_to_array(image))\n",
    "        y_trainarray.append(i)\n",
    "X_train = np.array(X_trainarray)\n",
    "y_train = np.array(y_trainarray)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The above process is repeated on the test dataset and X_test and y_test arrays are formed.\n",
    "\n",
    "testpathlist = list(testpath.iterdir())\n",
    "testclasslist = []\n",
    "for i in testpathlist:\n",
    "    length = len(i.parts)-1\n",
    "    testclasslist.append(i.parts[length])\n",
    "    \n",
    "subtestpath = []\n",
    "for i in testclasslist:\n",
    "    subtestpath.append(pathlib.Path(str(testpath)+'/'+ i))\n",
    "\n",
    "testinterlist = []\n",
    "listlength=0\n",
    "X_testarray=[]\n",
    "y_testarray = []\n",
    "for i in range(0,9):\n",
    "    testinterlist = list(subtestpath[i].iterdir())\n",
    "    listlength = listlength + len(testinterlist)\n",
    "    print(\"Label Name = \" + str(subtestpath[i].name))\n",
    "    print(\"Label Number = \" + str(i))\n",
    "    print(\"number of test images = \" + str(len(testinterlist)))\n",
    "    print()\n",
    "    for j in testinterlist:\n",
    "        image = tf.keras.utils.load_img(j)\n",
    "        image = image.resize((180,180))\n",
    "        X_testarray.append(tf.keras.utils.img_to_array(image))\n",
    "        y_testarray.append(i)\n",
    "X_test = np.array(X_testarray)\n",
    "y_test = np.array(y_testarray)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the X_train and X_test datasets.\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalise\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "## One Hot Encoding the y_train and y_test datasets\n",
    "y_train = tf.keras.utils.to_categorical(y_train,9)\n",
    "y_test = tf.keras.utils.to_categorical(y_test,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since google colab was to be used considering the GPU requirement. The above numpy arrays were saved on local drive\n",
    "## and then uploaded to google drive. These numpy arrays were then used in the code on google colab for further model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below code is used to save the numpy arrays on the local drive\n",
    "np.save(r\"D:\\Siddharth Upgrad\\Case Study\\17. Melanoma detection\\CNN_assignment\\X_train.npy\",X_train)\n",
    "np.save(r\"D:\\Siddharth Upgrad\\Case Study\\17. Melanoma detection\\CNN_assignment\\X_test.npy\",X_test)\n",
    "np.save(r\"D:\\Siddharth Upgrad\\Case Study\\17. Melanoma detection\\CNN_assignment\\y_train.npy\",y_train)\n",
    "np.save(r\"D:\\Siddharth Upgrad\\Case Study\\17. Melanoma detection\\CNN_assignment\\y_test.npy\",y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLfcXcZ9LjGv"
   },
   "outputs": [],
   "source": [
    "## Defining the batch size and the epochs\n",
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5f5y43GPog1"
   },
   "source": [
    "Use 80% of the images for training, and 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the numpy arrays to google colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "X_train = np.load(\"/content/drive/MyDrive/Assignment_Numpy/X_train.npy\")\n",
    "y_train = np.load(\"/content/drive/MyDrive/Assignment_Numpy/y_train.npy\")\n",
    "X_test = np.load(\"/content/drive/MyDrive/Assignment_Numpy/X_test.npy\")\n",
    "y_test = np.load(\"/content/drive/MyDrive/Assignment_Numpy/y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1,X_val,y_train1,y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JEAF6-sRyz8"
   },
   "source": [
    "### Create the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ync9xoW7GZgn"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Build Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(5,5),padding='same',strides=(1,1),activation='relu',input_shape=((180,180,3))))\n",
    "model.add(Conv2D(32,kernel_size=(5,5),padding='same',strides=(1,1),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3),padding='same',strides=(1,1),activation='relu'))\n",
    "model.add(Conv2D(64,kernel_size=(3,3),padding='same',strides=(1,1),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.05))\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=(3,3),padding='same',strides=(1,1),activation='relu'))\n",
    "model.add(Conv2D(128,kernel_size=(3,3),padding='same',strides=(1,1),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.05))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDKzJmHwSCtt"
   },
   "source": [
    "### Compile the model\n",
    "Choose an appropirate optimiser and loss function for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XB8wKtiPGe1j"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer = 'sgd',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZGWN4MZGhtJ"
   },
   "outputs": [],
   "source": [
    "# View the summary of all layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljD_83rwSl5O"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kkfw2rJXGlYC"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x = X_train1, y = y_train1,batch_size=batch_size,epochs=epochs,verbose=1,\n",
    "                    validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3679V8OShSE"
   },
   "source": [
    "### Visualizing training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1xkgk5nGubz"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvPphJYuSZhK"
   },
   "source": [
    "#### Todo: Write your findings after the model fit, see if there is an evidence of model overfit or underfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vRTPbJEn-pX"
   },
   "source": [
    "### Write your findings here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "22hljAl6GykA"
   },
   "outputs": [],
   "source": [
    "## The model has low training accuracy of about 74.5% and poor validation accuracy of 40% indicating a probable overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhNOKtSyUYzC"
   },
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-AUR_b7UcaK"
   },
   "source": [
    "#### Todo: Write your findings after the model fit, see if there is an evidence of model overfit or underfit. Do you think there is some improvement now as compared to the previous model run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TdDi4u-VTkW"
   },
   "source": [
    "#### **Todo:** Find the distribution of classes in the training dataset.\n",
    "#### **Context:** Many times real life datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAhwYgtTQRzq"
   },
   "outputs": [],
   "source": [
    "distributiondf = pd.DataFrame()\n",
    "distributiondf['Type'] = ['actinic keratosis','basal cell carcinoma','dermatofibroma','melanoma','nevus','pigmented benign keratosis',\n",
    "                          'seborrheic keratosis','squamous cell carcinoma','vascular lesion']\n",
    "distributiondf['Number_of_images'] = [traindf[0].sum(),traindf[1].sum(),traindf[2].sum(),traindf[3].sum(),traindf[4].sum(),\n",
    "                                          traindf[5].sum(),traindf[6].sum(),traindf[7].sum(),traindf[8].sum()]\n",
    "print(distributiondf)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.barplot(x=distributiondf['Type'],y=distributiondf['Number_of_images'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4csQL1dvO0b2"
   },
   "source": [
    "#### **Todo:** Write your findings here: \n",
    "\n",
    "#### - Which class has the least number of samples?\n",
    "   ### Answer: - Class \"pigmented benign keratosis\" has the highest number of samples\n",
    "   \n",
    "#### - Which classes dominate the data in terms proportionate number of samples?\n",
    "   ### Answer : - Classes \"Basal Cell Carcinoma\",\"melanoma\",\"pigmented bening keratosis,\"nevus\" dominate the data in terms of proportionate number of samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hb-stKyHPf8v"
   },
   "source": [
    "#### **Todo:** Rectify the class imbalance\n",
    "#### **Context:** You can use a python package known as `Augmentor` (https://augmentor.readthedocs.io/en/master/) to add more samples across all classes so that none of the classes have very few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItAg4rU-SzJh"
   },
   "outputs": [],
   "source": [
    "!pip install Augmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZKzTe3zWL4O"
   },
   "source": [
    "To use `Augmentor`, the following general procedure is followed:\n",
    "\n",
    "1. Instantiate a `Pipeline` object pointing to a directory containing your initial image data set.<br>\n",
    "2. Define a number of operations to perform on this data set using your `Pipeline` object.<br>\n",
    "3. Execute these operations by calling the `Pipeline’s` `sample()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['actinic keratosis','basal cell carcinoma','dermatofibroma','melanoma','nevus','pigmented benign keratosis',\n",
    "                          'seborrheic keratosis','squamous cell carcinoma','vascular lesion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below code scans every sub-folder in the training folder and augments the images. Total 500 samples are created in each class to remove the class imbalance issue.\n",
    "\n",
    "#### This code is executed in local machine and then similar to earlier the numpy arrays are created which are then imported to google drive and then used in the google colab code. The code is mentioned below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath= \"D:\\Siddharth Upgrad\\Case Study\\17. Melanoma detection\\CNN_assignment\\Skin cancer ISIC The International Skin Imaging Collaboration\\Train\"\n",
    "import Augmentor\n",
    "\n",
    "trainingpathlist = list(trainpath.iterdir())\n",
    "trainclasslist = []\n",
    "for i in trainpathlist:\n",
    "    length = len(i.parts)-1\n",
    "    trainclasslist.append(i.parts[length])\n",
    "    \n",
    "subtrainpath = []\n",
    "for i in trainclasslist:\n",
    "    subtrainpath.append(pathlib.Path(str(trainpath)+'/'+ i))\n",
    "\n",
    "for i in range(0,9):\n",
    "    p = Augmentor.Pipeline(subtrainpath[i])\n",
    "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
    "    p.sample(500) ## We are adding 500 samples per class to make sure that none of the classes are sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = pathlib.Path(r\"D:\\Siddharth Upgrad\\Case Study\\17. Melanoma detection\\CNN_assignment\\Augmented Train\")\n",
    "\n",
    "trainpathlist = list(trainpath.iterdir())\n",
    "trainclasslist = []\n",
    "for i in trainpathlist:\n",
    "    length = len(i.parts)-1\n",
    "    trainclasslist.append(i.parts[length])\n",
    "    \n",
    "subtrainpath = []\n",
    "for i in trainclasslist:\n",
    "    subtrainpath.append(pathlib.Path(str(trainpath)+'/'+ i))\n",
    "\n",
    "traininterlist = []\n",
    "listlength=0\n",
    "X_trainarray=[]\n",
    "y_trainarray = []\n",
    "for i in range(0,9):\n",
    "    traininterlist = list(subtrainpath[i].iterdir())\n",
    "    listlength = listlength + len(traininterlist)\n",
    "    print(\"Label Name = \" + str(subtrainpath[i].name))\n",
    "    print(\"Label Number = \" + str(i))\n",
    "    print(\"number of train images = \" + str(len(traininterlist)))\n",
    "    print()\n",
    "    for j in traininterlist:\n",
    "        image = tf.keras.utils.load_img(j)\n",
    "        image = image.resize((180,180))\n",
    "        X_trainarray.append(tf.keras.utils.img_to_array(image))\n",
    "        y_trainarray.append(i)\n",
    "X_train2 = np.array(X_trainarray)\n",
    "y_train2 = np.array(y_trainarray)\n",
    "\n",
    "print(X_train2.shape)\n",
    "print(y_train2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train2.astype('float32')\n",
    "X_train2 /= 255\n",
    "y_train2 = tf.keras.utils.to_categorical(y_train2,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(r\"D:\\Siddharth Upgrad\\Case Study\\17. Melanoma detection\\CNN_assignment\\Augmented_X_train.npy\",X_train2)\n",
    "np.save(r\"D:\\Siddharth Upgrad\\Case Study\\17. Melanoma detection\\CNN_assignment\\Augmented_y_train.npy\",y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING THE AUGMENTED NUMPY ARRAYS TO GOOGLE COLAB\n",
    "X_train2 = np.load(\"/content/drive/MyDrive/Assignment_Numpy/Augmented_X_train.npy\")\n",
    "y_train2 = np.load(\"/content/drive/MyDrive/Assignment_Numpy/Augmented_y_train.npy\")\n",
    "X_test2 = np.load(\"/content/drive/MyDrive/Assignment_Numpy/X_test.npy\")\n",
    "y_test2 = np.load(\"/content/drive/MyDrive/Assignment_Numpy/y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATING TRAIN AND VALIDATION SPLIT FOR THE AUGMENTED DATA\n",
    "X_train1,X_val,y_train1,y_val = train_test_split(X_train2,y_train2,test_size=0.2,random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaoWeOEpVjqH"
   },
   "source": [
    "#### **Todo:** Create your model (make sure to include normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ch0MuKvFVr7O"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Build Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3),padding='same',strides=(1,1),activation='relu',input_shape=((180,180,3))))\n",
    "model.add(Conv2D(64,kernel_size=(3,3),padding='same',strides=(1,1),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=(3,3),padding='same',strides=(1,1),activation='relu'))\n",
    "model.add(Conv2D(128,kernel_size=(3,3),padding='same',strides=(1,1),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv2D(256,kernel_size=(3,3),padding='same',strides=(1,1),activation='relu'))\n",
    "model.add(Conv2D(256,kernel_size=(3,3),padding='same',strides=(1,1),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bu5N9LxkVx1B"
   },
   "source": [
    "#### **Todo:** Compile your model (Choose optimizer and loss function appropriately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H47GWmLbdRW1"
   },
   "outputs": [],
   "source": [
    "## your code goes here\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer = 'sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gS-Y1bJV7uy"
   },
   "source": [
    "#### **Todo:**  Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcV6OdI4dRW1"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x = X_train1, y = y_train1,batch_size=batch_size,epochs=epochs,verbose=1,\n",
    "                    validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuvfCTsBWLMp"
   },
   "source": [
    "#### **Todo:**  Visualize the model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCTXwfkTdRW1"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Way4lakC4_p0"
   },
   "source": [
    "#### **Todo:**  Analyze your results here. Did you get rid of underfitting/overfitting? Did class rebalance help?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nV2BHg1dWrdY"
   },
   "outputs": [],
   "source": [
    "## When the above model is executed the train accuracy has improved to around 94% and validation accuracy is improved to around 84%.\n",
    "## This shows a significance improvement in the overall model due to class rebalance.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Starter code Assignment CNN Skin Cancer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
